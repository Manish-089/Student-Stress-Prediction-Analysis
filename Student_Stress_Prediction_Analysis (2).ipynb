{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b274a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a3d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "blue_palette = sns.color_palette(\"Blues_r\", n_colors=8)\n",
    "green_palette = sns.color_palette(\"Greens_r\", n_colors=8)\n",
    "orange_palette = sns.color_palette(\"Oranges_r\", n_colors=8)\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"StressLevelDataset.csv\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Total Records: {df.shape[0]}\")\n",
    "print(f\"Total Features: {df.shape[1]}\")\n",
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found in the dataset!\")\n",
    "else:\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate Rows: {duplicates}\")\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"UNIVARIATE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(7, 3, figsize=(20, 28))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(df.columns):\n",
    "    axes[idx].hist(df[col], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "    mean_val = df[col].mean()\n",
    "    median_val = df[col].median()\n",
    "    axes[idx].axvline(mean_val, color='darkgreen', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[idx].axvline(median_val, color='darkorange', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSkewness of Features:\")\n",
    "skewness = df.skew()\n",
    "print(skewness.sort_values(ascending=False))\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TARGET VARIABLE ANALYSIS (stress_level)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "stress_counts = df['stress_level'].value_counts().sort_index()\n",
    "bars = plt.bar(stress_counts.index, stress_counts.values, edgecolor='black', color='teal', alpha=0.8)\n",
    "plt.xlabel('Stress Level', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Distribution of Stress Levels', fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(stress_counts.values):\n",
    "    plt.text(stress_counts.index[i], v + 5, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "plt.savefig('stress_level_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStress Level Value Counts:\")\n",
    "print(stress_counts)\n",
    "print(f\"\\nPercentage Distribution:\")\n",
    "print((stress_counts / len(df) * 100).round(2))\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f',\n",
    "            cmap='YlGnBu', center=0, square=True, linewidths=1,\n",
    "            cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "stress_correlations = correlation_matrix['stress_level'].drop('stress_level').sort_values(ascending=False)\n",
    "print(\"\\nTop 10 Features Correlated with Stress Level:\")\n",
    "print(stress_correlations.head(10))\n",
    "print(\"\\nBottom 10 Features Correlated with Stress Level:\")\n",
    "print(stress_correlations.tail(10))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['darkgreen' if x > 0 else 'darkorange' for x in stress_correlations]\n",
    "stress_correlations.plot(kind='barh', color=colors)\n",
    "plt.xlabel('Correlation with Stress Level', fontsize=12)\n",
    "plt.title('Feature Correlations with Stress Level', fontsize=14, fontweight='bold')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('stress_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MULTICOLLINEARITY DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:  # Threshold of 0.7\n",
    "            high_corr_pairs.append({\n",
    "                'Feature 1': correlation_matrix.columns[i],\n",
    "                'Feature 2': correlation_matrix.columns[j],\n",
    "                'Correlation': correlation_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', ascending=False)\n",
    "    print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "    print(high_corr_df)\n",
    "else:\n",
    "    print(\"\\nNo highly correlated feature pairs found (threshold: 0.7)\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BIVARIATE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "top_features = stress_correlations.abs().nlargest(8).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    data_to_plot = [df[df['stress_level'] == level][feature].values\n",
    "                    for level in sorted(df['stress_level'].unique())]\n",
    "\n",
    "    bp = axes[idx].boxplot(data_to_plot, patch_artist=True)\n",
    "\n",
    "    colors = plt.cm.Blues(np.linspace(0.4, 0.9, len(bp['boxes'])))\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "    for element in ['whiskers', 'fliers', 'medians', 'caps']:\n",
    "        plt.setp(bp[element], color='darkblue')\n",
    "\n",
    "    axes[idx].set_title(f'{feature} vs Stress Level', fontsize=12)\n",
    "    axes[idx].set_xlabel('Stress Level')\n",
    "    axes[idx].set_ylabel(feature)\n",
    "    axes[idx].set_xticklabels(sorted(df['stress_level'].unique()))\n",
    "\n",
    "plt.suptitle('Top Features vs Stress Level', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_vs_stress_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OUTLIER DETECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "outlier_summary = {}\n",
    "for col in df.columns:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "    outlier_summary[col] = {\n",
    "        'count': len(outliers),\n",
    "        'percentage': (len(outliers) / len(df)) * 100\n",
    "    }\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).T\n",
    "outlier_df = outlier_df.sort_values('count', ascending=False)\n",
    "print(\"\\nOutlier Summary (IQR Method):\")\n",
    "print(outlier_df[outlier_df['count'] > 0])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "outlier_df[outlier_df['count'] > 0]['percentage'].plot(kind='bar', color='seagreen', alpha=0.8)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Percentage of Outliers', fontsize=12)\n",
    "plt.title('Percentage of Outliers by Feature', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outlier_percentages.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "X = df.drop('stress_level', axis=1)\n",
    "y = df['stress_level']\n",
    "\n",
    "mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "mi_scores_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'MI Score': mi_scores\n",
    "}).sort_values('MI Score', ascending=False)\n",
    "\n",
    "print(\"\\nMutual Information Scores:\")\n",
    "print(mi_scores_df)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = plt.cm.Oranges(np.linspace(0.4, 0.9, len(mi_scores_df)))\n",
    "plt.barh(mi_scores_df['Feature'], mi_scores_df['MI Score'], color=colors)\n",
    "plt.xlabel('Mutual Information Score', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.title('Feature Importance based on Mutual Information', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mutual_information_scores.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIMENSIONALITY REDUCTION (PCA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, 'o-',\n",
    "         color='steelblue', markersize=8, linewidth=2)\n",
    "ax1.set_xlabel('Principal Component', fontsize=12)\n",
    "ax1.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "ax1.set_title('PCA Scree Plot', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'o-',\n",
    "         color='darkgreen', markersize=8, linewidth=2)\n",
    "ax2.axhline(y=0.95, color='darkorange', linestyle='--', linewidth=2, label='95% Variance')\n",
    "ax2.set_xlabel('Number of Components', fontsize=12)\n",
    "ax2.set_ylabel('Cumulative Explained Variance Ratio', fontsize=12)\n",
    "ax2.set_title('Cumulative Explained Variance', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pca_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "print(f\"\\nNumber of components needed for 95% variance: {n_components_95}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICAL TESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "statistic, p_value = stats.normaltest(df['stress_level'])\n",
    "print(f\"\\nNormality Test for Stress Level:\")\n",
    "print(f\"Statistic: {statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Is normally distributed? {'Yes' if p_value > 0.05 else 'No'}\")\n",
    "\n",
    "print(\"\\nANOVA Tests (Feature groups by stress level):\")\n",
    "for feature in ['anxiety_level', 'depression', 'academic_performance']:\n",
    "    groups = [group[feature].values for name, group in df.groupby('stress_level')]\n",
    "    f_stat, p_val = stats.f_oneway(*groups)\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"  P-value: {p_val:.4f}\")\n",
    "    print(f\"  Significant difference? {'Yes' if p_val < 0.05 else 'No'}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDATIONS FOR ML PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. FEATURE SCALING:\")\n",
    "print(\"   - All features are on similar scales (mostly 0-5 range)\")\n",
    "print(\"   - StandardScaler or MinMaxScaler recommended for algorithms sensitive to scale\")\n",
    "print(\"   - Tree-based models may not require scaling\")\n",
    "\n",
    "print(\"\\n2. HANDLING MULTICOLLINEARITY:\")\n",
    "if high_corr_pairs:\n",
    "    print(\"   - Consider removing one feature from highly correlated pairs\")\n",
    "    print(\"   - Or use dimensionality reduction techniques (PCA, LDA)\")\n",
    "else:\n",
    "    print(\"   - No severe multicollinearity detected\")\n",
    "\n",
    "print(\"\\n3. OUTLIER TREATMENT:\")\n",
    "if outlier_df[outlier_df['count'] > 0].shape[0] > 0:\n",
    "    print(\"   - Consider capping outliers or using robust scaling\")\n",
    "    print(\"   - Tree-based models are generally robust to outliers\")\n",
    "else:\n",
    "    print(\"   - Minimal outliers detected\")\n",
    "\n",
    "print(\"\\n4. FEATURE ENGINEERING SUGGESTIONS:\")\n",
    "print(\"   - Create interaction features between highly correlated variables\")\n",
    "print(\"   - Consider polynomial features for non-linear relationships\")\n",
    "print(\"   - Group similar features (e.g., physical symptoms, academic factors)\")\n",
    "\n",
    "print(\"\\n5. CLASS IMBALANCE:\")\n",
    "stress_dist = df['stress_level'].value_counts(normalize=True)\n",
    "if stress_dist.min() < 0.1:\n",
    "    print(\"   - Consider using SMOTE or class weights for imbalanced classes\")\n",
    "else:\n",
    "    print(\"   - Classes are reasonably balanced\")\n",
    "\n",
    "print(\"\\n6. FEATURE SELECTION:\")\n",
    "print(\"   - Use mutual information scores for initial feature selection\")\n",
    "print(\"   - Consider recursive feature elimination with cross-validation\")\n",
    "print(f\"   - Start with top {len(mi_scores_df[mi_scores_df['MI Score'] > 0.1])} features based on MI scores\")\n",
    "\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "STRESS LEVEL DATASET ANALYSIS SUMMARY\n",
    "=====================================\n",
    "\n",
    "Dataset Overview:\n",
    "- Total Records: {df.shape[0]}\n",
    "- Total Features: {df.shape[1]}\n",
    "- No Missing Values: {missing_values.sum() == 0}\n",
    "- Duplicate Rows: {duplicates}\n",
    "\n",
    "Target Variable Distribution:\n",
    "{stress_counts.to_dict()}\n",
    "\n",
    "Top 5 Features Correlated with Stress Level:\n",
    "{stress_correlations.head(5).to_dict()}\n",
    "\n",
    "Feature Importance (Top 5 by Mutual Information):\n",
    "{mi_scores_df.head(5).to_dict()}\n",
    "\n",
    "Dimensionality Reduction:\n",
    "- Components for 95% variance: {n_components_95}\n",
    "\n",
    "Outliers Detected:\n",
    "- Features with >5% outliers: {len(outlier_df[outlier_df['percentage'] > 5])}\n",
    "\n",
    "Preprocessing Recommendations:\n",
    "1. Scaling: Recommended (StandardScaler/MinMaxScaler)\n",
    "2. Feature Selection: Start with top {len(mi_scores_df[mi_scores_df['MI Score'] > 0.1])} features\n",
    "3. Handle multicollinearity if needed\n",
    "4. Consider ensemble methods for robustness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddfed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('StressLevelDataset.csv')\n",
    "\n",
    "X = df.drop(columns=[\"stress_level\"])\n",
    "y = df[\"stress_level\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "with open('stress_survey_analysis_summary.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"- feature_distributions.png\")\n",
    "print(\"- stress_experience_distribution.png\")\n",
    "print(\"- stress_types_distribution.png\")\n",
    "print(\"- correlation_matrix.png\")\n",
    "print(\"- stress_correlations.png\")\n",
    "print(\"- feature_vs_stress_boxplots.png\")\n",
    "print(\"- outlier_percentages.png\")\n",
    "print(\"- mutual_information_scores.png\")\n",
    "print(\"- pca_analysis.png\")\n",
    "print(\"- stress_survey_analysis_summary.txt\")\n",
    "print(\"\\nReady for ML modeling!\")\n",
    "\n",
    "\n",
    "notebook['cells'].append(nbformat.v4.new_code_cell(ml_code))\n",
    "\n",
    "updated_path = Path(\"/mnt/data/comprehensive-analysis-student-stress-datasets-ML.ipynb\")\n",
    "with open(updated_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbformat.write(notebook, f)\n",
    "\n",
    "updated_path\n",
    "\n",
    "\n",
    "\n",
    "target_column = 'Which type of stress do you primarily experience?'  # TODO: Change this to your actual target column\n",
    "\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ecd3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb3b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba9082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3cbd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f33e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75dd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80becc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"SVM\": SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for name, model in models.items():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results.append((name, acc))\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\"]).sort_values(by=\"Accuracy\", ascending=False)\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646698a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(results_df[\"Model\"], results_df[\"Accuracy\"], color='skyblue')\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
